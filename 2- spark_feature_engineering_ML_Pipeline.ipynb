{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Project Step-by-Step: Part 2\n",
    "\n",
    "This notebook will walk you through 2 more steps in the ML lifecycle - **Feature Engineering** and **Model Fitting & Evaluation**.<br>\n",
    "* In the feature engineering part we'll see how to perform common aggregates using analytical functions.\n",
    "* In the modelling part we'll see how to prepare data for modelling in PySpark, and how to fit a model using MLLib.\n",
    "* Finally, we'll see how we can evaluate the model we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spark web interface is available at: http://DESKTOP-I19RH8M:4040\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"iot\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Get the URL of the Spark web interface\n",
    "web_ui_url = spark.sparkContext.uiWebUrl\n",
    "print(f\"The Spark web interface is available at: {web_ui_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+--------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+\n",
      "|                 ts|               uid|      source_ip|source_port|        dest_ip|dest_port|proto|service| duration|orig_bytes|resp_bytes|conn_state| history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|    label|      detailed-label|                 dt|                day|               hour|             minute|             second|is_bad|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+--------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+\n",
      "|1.526075739042012E9|CKIvQf4fqlhvM4j5n7|192.168.100.103|    40912.0| 132.16.185.104|  15258.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|       S|      1.0|         60.0|      0.0|          0.0|   Benign|                NULL|2018-05-11 23:55:39|2018-05-11 00:00:00|2018-05-11 23:00:00|2018-05-11 23:55:00|2018-05-11 23:55:39|     0|\n",
      "|1.526075725655915E9|CMszxf43boIxPptZEc|192.168.100.103|    50256.0|   210.6.131.76|     23.0|  tcp|missing|13.983001|      85.0|     183.0|        SF|ShAdDafF|     28.0|       1573.0|     23.0|       1399.0|Malicious|PartOfAHorizontal...|2018-05-11 23:55:25|2018-05-11 00:00:00|2018-05-11 23:00:00|2018-05-11 23:55:00|2018-05-11 23:55:25|     1|\n",
      "|1.526075685012671E9|CIagio3aHkqvsD8ED7|192.168.100.103|    43763.0| 243.203.70.134|  33800.0|  udp|missing|-999999.0| -999999.0| -999999.0|        S0|       D|      1.0|         40.0|      0.0|          0.0|   Benign|                NULL|2018-05-11 23:54:45|2018-05-11 00:00:00|2018-05-11 23:00:00|2018-05-11 23:54:00|2018-05-11 23:54:45|     0|\n",
      "|1.526075740021948E9|CLBvCX1oTdbQyImKbb|192.168.100.103|    48170.0|124.148.200.191|     23.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|       S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-11 23:55:40|2018-05-11 00:00:00|2018-05-11 23:00:00|2018-05-11 23:55:00|2018-05-11 23:55:40|     1|\n",
      "|1.526075686012848E9|Cxbg2N1vEvYe6NPcGj|192.168.100.103|    43763.0| 28.137.227.205|   8908.0|  udp|missing|-999999.0| -999999.0| -999999.0|        S0|       D|      1.0|         40.0|      0.0|          0.0|   Benign|                NULL|2018-05-11 23:54:46|2018-05-11 00:00:00|2018-05-11 23:00:00|2018-05-11 23:54:00|2018-05-11 23:54:46|     0|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+--------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"processed.pq\").withColumn(\n",
    "    \"is_bad\", F.when(F.col(\"label\") != \"Benign\", 1).otherwise(0)\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Since we have a time-component to this data, we can engineer all sorts of rolling features. The ones that I'll cover here are:\n",
    "* Number of times we've seen this source IP in the last minute\n",
    "* Number of times we've seen this destination IP in the last minute\n",
    "* Number of times we've seen this source PORT in the last minute\n",
    "* Number of times we've seen this destination PORT in the last minute\n",
    "\n",
    "To calculate these features, we'll need to use analytical functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mins_to_secs(mins):\n",
    "    return mins * 60\n",
    "\n",
    "\n",
    "def generate_window(window_in_minutes: int, partition_by: str, timestamp_col: str):\n",
    "    window = (\n",
    "        Window()\n",
    "        .partitionBy(F.col(partition_by))\n",
    "        .orderBy(F.col(timestamp_col).cast(\"long\"))\n",
    "        .rangeBetween(-mins_to_secs(window_in_minutes), -1)\n",
    "    )\n",
    "\n",
    "    return window\n",
    "\n",
    "\n",
    "def generate_rolling_aggregate(\n",
    "    col: str,\n",
    "    partition_by: str | None = None,\n",
    "    operation: str = \"count\",\n",
    "    timestamp_col: str = \"dt\",\n",
    "    window_in_minutes: int = 1,\n",
    "):\n",
    "    if partition_by is None:\n",
    "        partition_by = col\n",
    "\n",
    "    match operation:\n",
    "        case \"count\":\n",
    "            return F.count(col).over(\n",
    "                generate_window(\n",
    "                    window_in_minutes=window_in_minutes,\n",
    "                    partition_by=col,\n",
    "                    timestamp_col=timestamp_col,\n",
    "                )\n",
    "            )\n",
    "        case \"sum\":\n",
    "            return F.sum(col).over(\n",
    "                generate_window(\n",
    "                    window_in_minutes=window_in_minutes,\n",
    "                    partition_by=col,\n",
    "                    timestamp_col=timestamp_col,\n",
    "                )\n",
    "            )\n",
    "        case \"avg\":\n",
    "            return F.avg(col).over(\n",
    "                generate_window(\n",
    "                    window_in_minutes=window_in_minutes,\n",
    "                    partition_by=col,\n",
    "                    timestamp_col=timestamp_col,\n",
    "                )\n",
    "            )\n",
    "        case _:\n",
    "            raise ValueError(f\"Operation {operation} is not defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Rolling Count Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the nicely defined functions above, generating rolling averages and counts is a piece of cake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumns({\n",
    "    \"source_ip_count_last_min\": generate_rolling_aggregate(col=\"source_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_ip_count_last_30_mins\": generate_rolling_aggregate(col=\"source_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"source_port_count_last_min\": generate_rolling_aggregate(col=\"source_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_port_count_last_30_mins\": generate_rolling_aggregate(col=\"source_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"dest_ip_count_last_min\": generate_rolling_aggregate(col=\"dest_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"dest_ip_count_last_30_mins\": generate_rolling_aggregate(col=\"dest_ip\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"dest_port_count_last_min\": generate_rolling_aggregate(col=\"dest_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"dest_port_count_last_30_mins\": generate_rolling_aggregate(col=\"dest_port\", operation=\"count\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"source_ip_avg_pkts_last_min\": generate_rolling_aggregate(col=\"orig_pkts\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_ip_avg_pkts_last_30_mins\": generate_rolling_aggregate(col=\"orig_pkts\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "    \"source_ip_avg_bytes_last_min\": generate_rolling_aggregate(col=\"orig_ip_bytes\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=1),\n",
    "    \"source_ip_avg_bytes_last_30_mins\": generate_rolling_aggregate(col=\"orig_ip_bytes\", partition_by=\"source_ip\", operation=\"avg\", timestamp_col=\"dt\", window_in_minutes=30),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Data with Parquet Files\n",
    "\n",
    "In this section, we discuss the importance of saving your data after performing heavy processing operations, such as feature engineering. This ensures efficiency and avoids redundant computations. Here's the advice provided:\n",
    "\n",
    "- **Lazy Evaluation in PySpark**: PySpark uses lazy evaluation, meaning it doesn't execute computations until an action (e.g., show, collect) is called. This can be inefficient during interactive analysis as it recomputes data each time an action is invoked.\n",
    "- **Save Data After Heavy Processing**: After performing computationally intensive tasks, save the DataFrame to a Parquet file. This step ensures that subsequent operations on the data are faster since they avoid redoing previous computations.\n",
    "Overwrite Mode: Use the overwrite mode when writing to a Parquet file to ensure existing files are updated without errors.\n",
    "By following this approach, you streamline the data processing workflow and enhance performance during interactive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+------------------------+----------------------------+--------------------------+------------------------------+----------------------+--------------------------+------------------------+----------------------------+---------------------------+-------------------------------+----------------------------+--------------------------------+\n",
      "|                 ts|               uid|      source_ip|source_port|        dest_ip|dest_port|proto|service| duration|orig_bytes|resp_bytes|conn_state|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|    label|      detailed-label|                 dt|                day|               hour|             minute|             second|is_bad|source_ip_count_last_min|source_ip_count_last_30_mins|source_port_count_last_min|source_port_count_last_30_mins|dest_ip_count_last_min|dest_ip_count_last_30_mins|dest_port_count_last_min|dest_port_count_last_30_mins|source_ip_avg_pkts_last_min|source_ip_avg_pkts_last_30_mins|source_ip_avg_bytes_last_min|source_ip_avg_bytes_last_30_mins|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+------------------------+----------------------------+--------------------------+------------------------------+----------------------+--------------------------+------------------------+----------------------------+---------------------------+-------------------------------+----------------------------+--------------------------------+\n",
      "|1.525882699039411E9|CHkwjm1vWg0hc4KPt1|192.168.100.103|    59393.0| 108.111.88.163|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-09 18:18:19|2018-05-09 00:00:00|2018-05-09 18:00:00|2018-05-09 18:18:00|2018-05-09 18:18:19|     1|                     161|                        4756|                         1|                             1|                     1|                         1|                      17|                         678|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525889467143584E9|CeHU891Y9O7nGPmWa1|192.168.100.103|    34942.0| 173.122.46.230|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-09 20:11:07|2018-05-09 00:00:00|2018-05-09 20:00:00|2018-05-09 20:11:00|2018-05-09 20:11:07|     1|                     156|                        4847|                         1|                             1|                     1|                         1|                      22|                         651|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525891465088897E9|CuH1124KHoLIIBSB62|192.168.100.103|    49292.0|   173.120.2.67|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-09 20:44:25|2018-05-09 00:00:00|2018-05-09 20:00:00|2018-05-09 20:44:00|2018-05-09 20:44:25|     1|                     157|                        4800|                         1|                             1|                     1|                         1|                      18|                         738|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525895340047055E9| C3AbTQtWK1SJdSBZi|192.168.100.103|    51205.0|   173.118.8.28|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-09 21:49:00|2018-05-09 00:00:00|2018-05-09 21:00:00|2018-05-09 21:49:00|2018-05-09 21:49:00|     1|                     157|                        4747|                         1|                             1|                     1|                         1|                      19|                         689|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525912173044838E9| CHgntgTJRRO17cK48|192.168.100.103|    40084.0|173.125.176.116|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-10 02:29:33|2018-05-10 00:00:00|2018-05-10 02:00:00|2018-05-10 02:29:00|2018-05-10 02:29:33|     1|                     155|                        4583|                         1|                             1|                     1|                         1|                      25|                         661|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+------------------------+----------------------------+--------------------------+------------------------------+----------------------+--------------------------+------------------------+----------------------------+---------------------------+-------------------------------+----------------------------+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,execute and save the resulting table into a new parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"feature_engineered.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = spark.read.parquet(\"feature_engineered.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the speed of calling the old `df` vs the new `df_fe`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+------------------------+----------------------------+--------------------------+------------------------------+----------------------+--------------------------+------------------------+----------------------------+---------------------------+-------------------------------+----------------------------+--------------------------------+\n",
      "|                 ts|               uid|      source_ip|source_port|        dest_ip|dest_port|proto|service| duration|orig_bytes|resp_bytes|conn_state|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|    label|      detailed-label|                 dt|                day|               hour|             minute|             second|is_bad|source_ip_count_last_min|source_ip_count_last_30_mins|source_port_count_last_min|source_port_count_last_30_mins|dest_ip_count_last_min|dest_ip_count_last_30_mins|dest_port_count_last_min|dest_port_count_last_30_mins|source_ip_avg_pkts_last_min|source_ip_avg_pkts_last_30_mins|source_ip_avg_bytes_last_min|source_ip_avg_bytes_last_30_mins|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+------------------------+----------------------------+--------------------------+------------------------------+----------------------+--------------------------+------------------------+----------------------------+---------------------------+-------------------------------+----------------------------+--------------------------------+\n",
      "|1.525882699039411E9|CHkwjm1vWg0hc4KPt1|192.168.100.103|    59393.0| 108.111.88.163|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-09 18:18:19|2018-05-09 00:00:00|2018-05-09 18:00:00|2018-05-09 18:18:00|2018-05-09 18:18:19|     1|                     161|                        4756|                         1|                             1|                     1|                         1|                      17|                         678|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525889467143584E9|CeHU891Y9O7nGPmWa1|192.168.100.103|    34942.0| 173.122.46.230|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-09 20:11:07|2018-05-09 00:00:00|2018-05-09 20:00:00|2018-05-09 20:11:00|2018-05-09 20:11:07|     1|                     156|                        4847|                         1|                             1|                     1|                         1|                      22|                         651|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525891465088897E9|CuH1124KHoLIIBSB62|192.168.100.103|    49292.0|   173.120.2.67|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-09 20:44:25|2018-05-09 00:00:00|2018-05-09 20:00:00|2018-05-09 20:44:00|2018-05-09 20:44:25|     1|                     157|                        4800|                         1|                             1|                     1|                         1|                      18|                         738|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525895340047055E9| C3AbTQtWK1SJdSBZi|192.168.100.103|    51205.0|   173.118.8.28|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-09 21:49:00|2018-05-09 00:00:00|2018-05-09 21:00:00|2018-05-09 21:49:00|2018-05-09 21:49:00|     1|                     157|                        4747|                         1|                             1|                     1|                         1|                      19|                         689|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525912173044838E9| CHgntgTJRRO17cK48|192.168.100.103|    40084.0|173.125.176.116|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-10 02:29:33|2018-05-10 00:00:00|2018-05-10 02:00:00|2018-05-10 02:29:00|2018-05-10 02:29:33|     1|                     155|                        4583|                         1|                             1|                     1|                         1|                      25|                         661|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525925854083516E9|Cul4ai4CUbNAYanTg4|192.168.100.103|    50073.0|    173.5.28.39|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-10 06:17:34|2018-05-10 00:00:00|2018-05-10 06:00:00|2018-05-10 06:17:00|2018-05-10 06:17:34|     1|                     154|                        4603|                         1|                             1|                     1|                         1|                      23|                         579|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "| 1.52594149209772E9|CCLFAt4UzckQdp9yfb|192.168.100.103|    49029.0| 108.127.76.198|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-10 10:38:12|2018-05-10 00:00:00|2018-05-10 10:00:00|2018-05-10 10:38:00|2018-05-10 10:38:12|     1|                     155|                        4638|                         1|                             1|                     1|                         1|                      33|                         672|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525943112169118E9|Caz1sZ3aVodjrFT8M1|192.168.100.103|    49717.0|  99.200.143.39|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-10 11:05:12|2018-05-10 00:00:00|2018-05-10 11:00:00|2018-05-10 11:05:00|2018-05-10 11:05:12|     1|                     154|                        4645|                         1|                             1|                     1|                         1|                      15|                         629|                       NULL|                            0.0|                        NULL|                             0.0|\n",
      "|1.525955610460311E9|CAZB4u33HSjlwQRYj3|192.168.100.103|    54378.0| 181.37.172.172|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-10 14:33:30|2018-05-10 00:00:00|2018-05-10 14:00:00|2018-05-10 14:33:00|2018-05-10 14:33:30|     1|                     152|                        4587|                         1|                             1|                     1|                         1|                      37|                         698|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "|1.525979387110037E9| CP7YsK1qXnatRCVU1|192.168.100.103|    34508.0|174.156.142.237|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|     RSTRH|     ^r|      0.0|          0.0|      1.0|         40.0|Malicious|PartOfAHorizontal...|2018-05-10 21:09:47|2018-05-10 00:00:00|2018-05-10 21:00:00|2018-05-10 21:09:00|2018-05-10 21:09:47|     1|                     153|                        4553|                         1|                             1|                     1|                         1|                      21|                         620|                       NULL|                           NULL|                        NULL|                            NULL|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------+------------------------+----------------------------+--------------------------+------------------------------+----------------------+--------------------------+------------------------+----------------------------+---------------------------+-------------------------------+----------------------------+--------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fe.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a drastic difference is because when you call `df.show()` it's going to execute all of the very expensive operations we did. Instead, it's better to construct a new dataframe for the analysis. This method ensures that the heavy computations done during feature engineering are stored, allowing for quicker access and manipulation of the DataFrame in subsequent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ts', 'uid', 'source_ip', 'source_port', 'dest_ip']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fe.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"source_ip_count_last_min\",\n",
    "    \"source_ip_count_last_30_mins\",\n",
    "    \"source_port_count_last_min\",\n",
    "    \"source_port_count_last_30_mins\",\n",
    "    # \"dest_ip_count_last_min\",\n",
    "    # \"dest_ip_count_last_30_mins\",\n",
    "    # \"dest_port_count_last_min\",\n",
    "    # \"dest_port_count_last_30_mins\",\n",
    "    \"source_ip_avg_pkts_last_min\",\n",
    "    \"source_ip_avg_pkts_last_30_mins\",\n",
    "    \"source_ip_avg_bytes_last_min\",\n",
    "    \"source_ip_avg_bytes_last_30_mins\",\n",
    "]\n",
    "categorical_features = [\"proto\", \"service\", \"conn_state\", \"history\"]\n",
    "categorical_features_indexed = [c + \"_index\" for c in categorical_features]\n",
    "\n",
    "input_features = numerical_features + categorical_features_indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare categories\n",
    "\n",
    "In this section, we perform pre-processing on the categorical features to remove rare categories that appear infrequently. This step is crucial for optimizing machine learning model performance by reducing noise and potential overfitting. Here’s a breakdown of what the code does:\n",
    "\n",
    "1. **Count Unique Categories:** We start by counting the number of distinct values for each categorical feature.\n",
    "2. **Identify Frequent Categories:** For each categorical feature, we group the data by the feature, count the occurrences, and filter out categories that appear more than 100 times.\n",
    "3. **Replace Rare Categories:** We replace infrequent categories with a new value 'Other'.\n",
    "\n",
    "This process ensures that only the most relevant categories are retained, simplifying the model’s learning process and improving its generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------+--------------------------+-----------------------+\n",
      "|count(DISTINCT proto)|count(DISTINCT service)|count(DISTINCT conn_state)|count(DISTINCT history)|\n",
      "+---------------------+-----------------------+--------------------------+-----------------------+\n",
      "|                    3|                      6|                        12|                    167|\n",
      "+---------------------+-----------------------+--------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count distinct values in categorical features\n",
    "df_fe.select([F.count_distinct(c) for c in categorical_features]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store frequent categories\n",
    "categorical_valid_values = {}\n",
    "\n",
    "# Identify and replace rare categories for each feature\n",
    "for c in categorical_features:\n",
    "    # Find frequent values\n",
    "    categorical_valid_values[c] = (\n",
    "        df_fe.groupby(c)\n",
    "        .count()\n",
    "        .filter(F.col(\"count\") > 100)\n",
    "        .select(c)\n",
    "        .toPandas()\n",
    "        .values.ravel()\n",
    "    )\n",
    "\n",
    "# Replace rare categories with 'Other'\n",
    "    df_fe = df_fe.withColumn(\n",
    "        c,\n",
    "        F.when(F.col(c).isin(list(categorical_valid_values[c])), F.col(c)).otherwise(\n",
    "            F.lit(\"Other\").alias(c)\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------+--------------------------+-----------------------+\n",
      "|count(DISTINCT proto)|count(DISTINCT service)|count(DISTINCT conn_state)|count(DISTINCT history)|\n",
      "+---------------------+-----------------------+--------------------------+-----------------------+\n",
      "|                    3|                      4|                         8|                     23|\n",
      "+---------------------+-----------------------+--------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count distinct values after replacement\n",
    "df_fe.select([F.count_distinct(c) for c in categorical_features]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "Train test split will need to be done using the source IP address, otherwise we risk leaking data. The best way to do this is by splitting the IP addresses at random, and then filtering the data frame according to the IP address.\n",
    "\n",
    "\n",
    "In this section, we perform a strategic train-test split to ensure no data leakage, which is crucial for model performance evaluation. Instead of a random split, we split the data based on IP addresses to avoid including the same IP in both training and testing sets.\n",
    "\n",
    "Here's a summary of the process:\n",
    "\n",
    "1. **Group by `source_ip`:** First, we analyze the data by grouping by `source_ip` and summing the `is_bad` column to identify the number of malicious activities per IP.\n",
    "2. **Select Training IPs:** We exclude known malicious IPs and randomly select 80% of the remaining IPs for training. This ensures that our training set mostly contains non-malicious activity.\n",
    "3. **Join with Main DataFrame:** We join the selected training IPs back to the main DataFrame, marking them as part of the training set.\n",
    "4. **Include Malicious IPs:** We include one known malicious IP in both the training and testing sets to ensure the model is exposed to malicious activity during training.\n",
    "5. **Create Train and Test Sets:** Finally, we create the training and testing sets based on the marked IPs.\n",
    "\n",
    "This approach prevents data leakage and ensures that our model's evaluation is more realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|      source_ip|bad_sum|\n",
      "+---------------+-------+\n",
      "|192.168.100.103| 539473|\n",
      "|    192.168.2.5| 151566|\n",
      "|    192.168.2.1|      1|\n",
      "|173.216.214.224|      0|\n",
      "|   123.25.17.53|      0|\n",
      "+---------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fe.groupby(\"source_ip\").agg(F.sum(F.col(\"is_bad\")).alias(\"bad_sum\")).orderBy(\"bad_sum\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training non-malicious IPs (80%)\n",
    "train_ips = (\n",
    "    df_fe.where(\n",
    "        ~F.col(\"source_ip\").isin([\"192.168.100.103\", \"192.168.2.5\", \"192.168.2.1\"])\n",
    "    )\n",
    "    .select(F.col(\"source_ip\"), F.lit(1).alias(\"is_train\"))\n",
    "    .dropDuplicates()\n",
    "    .sample(0.8)\n",
    ")\n",
    "\n",
    "\n",
    "df_fe = df_fe.join(train_ips, \"source_ip\", \"left\")\n",
    "\n",
    "# Add 1 malicious IP to training and testing data\n",
    "df_train = df_fe.where((F.col(\"is_train\") == 1) | (F.col(\"source_ip\") == \"192.168.100.103\"))\n",
    "df_test = df_fe.where((F.col(\"is_train\") != 1) | (F.col(\"source_ip\") == \"192.168.2.5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Building the Machine Learning Pipeline\n",
    "\n",
    "In this section, we construct a machine learning pipeline using PySpark, which is different from scikit-learn but follows a similar concept of chaining preprocessing steps and the model into a single workflow. Our pipeline consists of three main stages:\n",
    "\n",
    "1. **String Indexer:** This step converts categorical features into numerical values. It functions similarly to a label encoder. We specify the input categorical columns and the corresponding output columns (with indexed names). If a new value is encountered that wasn't seen during training, it will be placed into a separate bin for unknown values.\n",
    "   \n",
    "2. **Vector Assembler:** This component consolidates all feature columns into a single vector column named `features`. PySpark's machine learning algorithms operate on these vectorized features.\n",
    "   \n",
    "3. **Random Forest Classifier:** We use a Random Forest model for classification. The label column is specified, and we set the number of trees to 100. Other parameters can be adjusted as needed.\n",
    "\n",
    "Here's the code to set up and run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = StringIndexer(inputCols=categorical_features, outputCols=categorical_features_indexed, handleInvalid='skip')\n",
    "va = VectorAssembler(inputCols=input_features, outputCol=\"features\", handleInvalid='skip' )\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"is_bad\", numTrees=100)\n",
    "\n",
    "pipeline = Pipeline(stages=[ind, va, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(df_train)\n",
    "test_preds = pipeline.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using this pipeline, we streamline the preprocessing and modeling steps, ensuring a consistent and reproducible workflow. The pipeline is first fit on the training data, and then it is used to transform the test data, which includes generating predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "After training our model, we need to evaluate its performance to understand how well it predicts malicious IP activity. In PySpark, the `BinaryClassificationEvaluator` is used for this purpose, providing metrics like Area Under the ROC Curve (ROC AUC) and Area Under the Precision-Recall Curve (PR AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we evaluate its performance using multiple metrics to gain a comprehensive understanding of its effectiveness. In this case, we use both ROC AUC and PR AUC metrics:\n",
    "\n",
    "- **ROC AUC (Receiver Operating Characteristic Area Under Curve)**: Provides an overall measure of the model's ability to distinguish between classes across various thresholds.\n",
    "\n",
    "- **PR AUC (Precision-Recall Area Under Curve)**: Focuses on the performance of the model specifically with regard to the positive class, which is especially important in cases where the classes are imbalanced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #d0d0d0; padding: 10px; background-color: #f9f9f9; border-radius: 5px;\">\n",
    "  <strong>Note:</strong>\n",
    "  <h3>Differences Between ROC AUC and PR AUC and When to Use Each</h3>\n",
    "  \n",
    "  <strong>ROC AUC (Receiver Operating Characteristic Area Under Curve)</strong>\n",
    "  <ul>\n",
    "    <li><strong>Definition</strong>: ROC AUC measures the ability of the model to distinguish between classes. The area under the ROC curve is used to evaluate the performance of a classification model.</li>\n",
    "    <li><strong>ROC Curve</strong>: It is a plot that shows the true positive rate (TPR) versus the false positive rate (FPR) at various classification thresholds.</li>\n",
    "    <li><strong>When to Use</strong>: It is more useful when the classes are balanced or the cost of false positives and false negatives is similar.</li>\n",
    "  </ul>\n",
    "  \n",
    "  <strong>PR AUC (Precision-Recall Area Under Curve)</strong>\n",
    "  <ul>\n",
    "    <li><strong>Definition</strong>: PR AUC measures the relationship between precision and recall. The area under the PR curve is used to evaluate the performance of a classification model, especially in imbalanced datasets.</li>\n",
    "    <li><strong>PR Curve</strong>: It is a plot that shows precision versus recall at various classification thresholds.</li>\n",
    "    <li><strong>When to Use</strong>: It is more useful when there is a significant imbalance in the classes (e.g., when the positive class is rare) and when the cost of false negatives is high.</li>\n",
    "  </ul>\n",
    "  \n",
    "  <strong>Summary</strong>\n",
    "  <ul>\n",
    "    <li><strong>ROC AUC</strong>: Use when the classes are balanced and you want to evaluate the overall ability of the model to distinguish between classes.</li>\n",
    "    <li><strong>PR AUC</strong>: Use when the classes are imbalanced and you want to evaluate the model's effectiveness in detecting the positive class.</li>\n",
    "  </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC 0.9746549938463899\n",
      "PR AUC 0.999508469266476\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "roc = BinaryClassificationEvaluator(labelCol=\"is_bad\", metricName=\"areaUnderROC\")\n",
    "print(\"ROC AUC\", roc.evaluate(test_preds))\n",
    "\n",
    "pr = BinaryClassificationEvaluator(labelCol=\"is_bad\", metricName=\"areaUnderPR\")\n",
    "print(\"PR AUC\", pr.evaluate(test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Understanding which features are most important to our model can help diagnose issues and improve performance. Random Forest models in PySpark provide feature importance scores, which we can extract and analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.300528</td>\n",
       "      <td>history_index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.242503</td>\n",
       "      <td>proto_index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.176689</td>\n",
       "      <td>source_port_count_last_30_mins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.104852</td>\n",
       "      <td>source_port_count_last_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.086021</td>\n",
       "      <td>source_ip_avg_bytes_last_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.038408</td>\n",
       "      <td>source_ip_avg_bytes_last_30_mins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038212</td>\n",
       "      <td>orig_ip_bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003548</td>\n",
       "      <td>source_ip_avg_pkts_last_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.003540</td>\n",
       "      <td>source_ip_avg_pkts_last_30_mins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001775</td>\n",
       "      <td>orig_pkts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001774</td>\n",
       "      <td>duration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001207</td>\n",
       "      <td>conn_state_index</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000646</td>\n",
       "      <td>orig_bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000105</td>\n",
       "      <td>resp_bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>source_ip_count_last_30_mins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000075</td>\n",
       "      <td>resp_ip_bytes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000019</td>\n",
       "      <td>source_ip_count_last_min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000012</td>\n",
       "      <td>resp_pkts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>service_index</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    importance                           feature\n",
       "18    0.300528                     history_index\n",
       "15    0.242503                       proto_index\n",
       "10    0.176689    source_port_count_last_30_mins\n",
       "9     0.104852        source_port_count_last_min\n",
       "13    0.086021      source_ip_avg_bytes_last_min\n",
       "14    0.038408  source_ip_avg_bytes_last_30_mins\n",
       "4     0.038212                     orig_ip_bytes\n",
       "11    0.003548       source_ip_avg_pkts_last_min\n",
       "12    0.003540   source_ip_avg_pkts_last_30_mins\n",
       "3     0.001775                         orig_pkts\n",
       "0     0.001774                          duration\n",
       "17    0.001207                  conn_state_index\n",
       "1     0.000646                        orig_bytes\n",
       "2     0.000105                        resp_bytes\n",
       "8     0.000084      source_ip_count_last_30_mins\n",
       "6     0.000075                     resp_ip_bytes\n",
       "7     0.000019          source_ip_count_last_min\n",
       "5     0.000012                         resp_pkts\n",
       "16    0.000001                     service_index"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"importance\": list(pipeline.stages[-1].featureImportances),\n",
    "        \"feature\": pipeline.stages[-2].getInputCols(),\n",
    "    }\n",
    ").sort_values(\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosing and Improving Model Performance\n",
    "Initial evaluation results might show discrepancies or unexpected performance metrics. For instance, we might find that the ROC AUC is lower than expected, while the PR AUC is higher. This could indicate issues with specific features.\n",
    "\n",
    "To diagnose and address these issues, we can examine the feature importances and perform further analysis on the data:\n",
    "\n",
    "- Check Feature Statistics: Compare the statistics of important features between malicious and non-malicious activities in both the training and test sets.\n",
    "- Re-evaluate Model Performance: Exclude problematic features from the model and retrain it to see if the performance improves.\n",
    "\n",
    "\n",
    "The sentence could be slightly revised for clarity and completeness. Here's a refined version:\n",
    "\n",
    "Removing Problematic Features: After analyzing the feature importances and their statistics, I decided to remove the problematic features created based on destination ports. I then retrained the model using only the categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of diagnosing feature importance and making adjustment\n",
    "# Grouping by 'is_bad' to compare feature statistics\n",
    "# df_train.groupBy(\"is_bad\").agg(\n",
    "#     F.avg(\"destination_port_count_30min\").alias(\"avg_dest_port_count_30min\"),\n",
    "#     F.avg(\"destination_port_count_1min\").alias(\"avg_dest_port_count_1min\")\n",
    "# ).show()\n",
    "\n",
    "# df_test.groupBy(\"is_bad\").agg(\n",
    "#     F.avg(\"destination_port_count_30min\").alias(\"avg_dest_port_count_30min\"),\n",
    "#     F.avg(\"destination_port_count_1min\").alias(\"avg_dest_port_count_1min\")\n",
    "# ).show()\n",
    "\n",
    "# # If the feature is problematic, exclude it and retrain the model\n",
    "# input_features.remove(\"destination_port_count_30min\")\n",
    "# input_features.remove(\"destination_port_count_1min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting PySpark Models and Pipelines\n",
    "We proceed to export both the model and the entire pipeline. Exporting the model alone can be done by accessing the trained model and using the save method, which stores the model's data and metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.stages[-1].save(\"rf_basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is often more effective to save the entire pipeline. This approach preserves not only the model but also the pre-processing steps, such as the StringIndexer and VectorAssembler, ensuring that all stages of data transformation and model training are included. To save a pipeline, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save(\"pipeline_basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method captures the entire workflow, making it easier to deploy or reuse the model and its associated transformations together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
